{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set the current working directory\n",
    "curr_path = os.getcwd().split(\"/\")[:-1]\n",
    "curr_path = \"/\".join(curr_path)\n",
    "os.chdir(curr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of nodes: 25\n",
      "Num. of edges: 210\n",
      "Num. of triangles: 770\n",
      "Shape: (25, 210, 770)\n",
      "Max Dimension: 2\n",
      "Generating coordinates using spring layout.\n",
      "Flow: 210\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# read dataset\n",
    "from sclibrary import load_dataset\n",
    "\n",
    "\n",
    "sc, _, flow = load_dataset()\"forex\")\n",
    "# get the y from flow dict\n",
    "y = np.fromiter(flow.values(), dtype=float)\n",
    "\n",
    "print(\"Flow:\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sclibrary.hogde_gp import HodgeGPTrainer\n",
    "\n",
    "B1 = sc.incidence_matrix(rank=1).toarray()\n",
    "B2 = sc.incidence_matrix(rank=2).toarray()\n",
    "L1 = sc.hodge_laplacian_matrix(rank=1).toarray()\n",
    "L1l = sc.lower_laplacian_matrix(rank=1).toarray()\n",
    "L1u = sc.upper_laplacian_matrix(rank=1).toarray()\n",
    "\n",
    "hogde_gp = HodgeGPTrainer(\n",
    "    B1 = B1,\n",
    "    B2 = B2,\n",
    "    L1 = L1,\n",
    "    L1l = L1l,\n",
    "    L1u = L1u,\n",
    "    y = y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1:  (210, 210)\n",
      "L1l:  (210, 210)\n",
      "L1u:  (210, 210)\n",
      "B1:  (25, 210)\n",
      "B2:  (210, 770)\n",
      "x_train: (42,)\n",
      "x_test: (168,)\n",
      "y_train: (42,)\n",
      "y_test: (168,)\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.2\n",
    "data_normalization = False\n",
    "\n",
    "laplacians = hogde_gp.get_laplacians()\n",
    "incidence_matrices = hogde_gp.get_incidence_matrices()\n",
    "eigenpairs = hogde_gp.get_eigenpairs()\n",
    "\n",
    "x_train, y_train, x_test, y_test, x, y  = hogde_gp.train_test_split(train_ratio=train_ratio, data_normalization=data_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(34.), tensor(4.4740))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sclibrary.hogde_gp.kernel_serializer import KernelSerializer\n",
    "\n",
    "kernel_type = \"matern\"\n",
    "data_name = \"forex\"\n",
    "\n",
    "kernel = KernelSerializer().serialize(eigenpairs=eigenpairs, kernel_type=kernel_type, data_name=data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from sclibrary.hogde_gp import ExactGPModel\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(x_train, y_train, likelihood, kernel, mean_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sclibrary.hogde_gp.forex.matern.MaternKernelForex"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "output_device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   model = model.to(output_device)\n",
    "   likelihood = likelihood.to(output_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise                   value = 0.0\n",
      "Parameter name: mean_module.raw_constant                           value = 0.0\n",
      "Parameter name: covar_module.raw_outputscale                       value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_kappa_down            value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_kappa_up              value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_kappa                 value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_mu                    value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_mu_down               value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_mu_up                 value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_h                     value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_h_down                value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_h_up                  value = 0.0\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:50} value = {param.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/1000 - Loss: 3.867 \n",
      "Iter 2/1000 - Loss: 3.571 \n",
      "Iter 3/1000 - Loss: 3.321 \n",
      "Iter 4/1000 - Loss: 3.112 \n",
      "Iter 5/1000 - Loss: 2.939 \n",
      "Iter 6/1000 - Loss: 2.795 \n",
      "Iter 7/1000 - Loss: 2.676 \n",
      "Iter 8/1000 - Loss: 2.577 \n",
      "Iter 9/1000 - Loss: 2.495 \n",
      "Iter 10/1000 - Loss: 2.426 \n",
      "Iter 11/1000 - Loss: 2.369 \n",
      "Iter 12/1000 - Loss: 2.322 \n",
      "Iter 13/1000 - Loss: 2.283 \n",
      "Iter 14/1000 - Loss: 2.251 \n",
      "Iter 15/1000 - Loss: 2.226 \n",
      "Iter 16/1000 - Loss: 2.206 \n",
      "Iter 17/1000 - Loss: 2.190 \n",
      "Iter 18/1000 - Loss: 2.179 \n",
      "Iter 19/1000 - Loss: 2.170 \n",
      "Iter 20/1000 - Loss: 2.164 \n",
      "Iter 21/1000 - Loss: 2.160 \n",
      "Iter 22/1000 - Loss: 2.157 \n",
      "Iter 23/1000 - Loss: 2.156 \n",
      "Iter 24/1000 - Loss: 2.155 \n",
      "Iter 25/1000 - Loss: 2.155 \n",
      "Iter 26/1000 - Loss: 2.155 \n",
      "Iter 27/1000 - Loss: 2.155 \n",
      "Iter 28/1000 - Loss: 2.155 \n",
      "Iter 29/1000 - Loss: 2.155 \n",
      "Iter 30/1000 - Loss: 2.155 \n",
      "Iter 31/1000 - Loss: 2.154 \n",
      "Iter 32/1000 - Loss: 2.153 \n",
      "Iter 33/1000 - Loss: 2.152 \n",
      "Iter 34/1000 - Loss: 2.151 \n",
      "Iter 35/1000 - Loss: 2.149 \n",
      "Iter 36/1000 - Loss: 2.147 \n",
      "Iter 37/1000 - Loss: 2.145 \n",
      "Iter 38/1000 - Loss: 2.142 \n",
      "Iter 39/1000 - Loss: 2.140 \n",
      "Iter 40/1000 - Loss: 2.137 \n",
      "Iter 41/1000 - Loss: 2.134 \n",
      "Iter 42/1000 - Loss: 2.130 \n",
      "Iter 43/1000 - Loss: 2.127 \n",
      "Iter 44/1000 - Loss: 2.124 \n",
      "Iter 45/1000 - Loss: 2.120 \n",
      "Iter 46/1000 - Loss: 2.117 \n",
      "Iter 47/1000 - Loss: 2.113 \n",
      "Iter 48/1000 - Loss: 2.110 \n",
      "Iter 49/1000 - Loss: 2.106 \n",
      "Iter 50/1000 - Loss: 2.103 \n",
      "Iter 51/1000 - Loss: 2.100 \n",
      "Iter 52/1000 - Loss: 2.096 \n",
      "Iter 53/1000 - Loss: 2.093 \n",
      "Iter 54/1000 - Loss: 2.090 \n",
      "Iter 55/1000 - Loss: 2.086 \n",
      "Iter 56/1000 - Loss: 2.083 \n",
      "Iter 57/1000 - Loss: 2.080 \n",
      "Iter 58/1000 - Loss: 2.077 \n",
      "Iter 59/1000 - Loss: 2.073 \n",
      "Iter 60/1000 - Loss: 2.070 \n",
      "Iter 61/1000 - Loss: 2.067 \n",
      "Iter 62/1000 - Loss: 2.064 \n",
      "Iter 63/1000 - Loss: 2.060 \n",
      "Iter 64/1000 - Loss: 2.057 \n",
      "Iter 65/1000 - Loss: 2.054 \n",
      "Iter 66/1000 - Loss: 2.050 \n",
      "Iter 67/1000 - Loss: 2.047 \n",
      "Iter 68/1000 - Loss: 2.043 \n",
      "Iter 69/1000 - Loss: 2.040 \n",
      "Iter 70/1000 - Loss: 2.036 \n",
      "Iter 71/1000 - Loss: 2.032 \n",
      "Iter 72/1000 - Loss: 2.028 \n",
      "Iter 73/1000 - Loss: 2.025 \n",
      "Iter 74/1000 - Loss: 2.021 \n",
      "Iter 75/1000 - Loss: 2.017 \n",
      "Iter 76/1000 - Loss: 2.013 \n",
      "Iter 77/1000 - Loss: 2.009 \n",
      "Iter 78/1000 - Loss: 2.005 \n",
      "Iter 79/1000 - Loss: 2.001 \n",
      "Iter 80/1000 - Loss: 1.997 \n",
      "Iter 81/1000 - Loss: 1.993 \n",
      "Iter 82/1000 - Loss: 1.989 \n",
      "Iter 83/1000 - Loss: 1.984 \n",
      "Iter 84/1000 - Loss: 1.980 \n",
      "Iter 85/1000 - Loss: 1.976 \n",
      "Iter 86/1000 - Loss: 1.972 \n",
      "Iter 87/1000 - Loss: 1.968 \n",
      "Iter 88/1000 - Loss: 1.963 \n",
      "Iter 89/1000 - Loss: 1.959 \n",
      "Iter 90/1000 - Loss: 1.955 \n",
      "Iter 91/1000 - Loss: 1.951 \n",
      "Iter 92/1000 - Loss: 1.947 \n",
      "Iter 93/1000 - Loss: 1.942 \n",
      "Iter 94/1000 - Loss: 1.938 \n",
      "Iter 95/1000 - Loss: 1.934 \n",
      "Iter 96/1000 - Loss: 1.930 \n",
      "Iter 97/1000 - Loss: 1.926 \n",
      "Iter 98/1000 - Loss: 1.922 \n",
      "Iter 99/1000 - Loss: 1.917 \n",
      "Iter 100/1000 - Loss: 1.913 \n",
      "Iter 101/1000 - Loss: 1.909 \n",
      "Iter 102/1000 - Loss: 1.905 \n",
      "Iter 103/1000 - Loss: 1.901 \n",
      "Iter 104/1000 - Loss: 1.897 \n",
      "Iter 105/1000 - Loss: 1.893 \n",
      "Iter 106/1000 - Loss: 1.889 \n",
      "Iter 107/1000 - Loss: 1.884 \n",
      "Iter 108/1000 - Loss: 1.880 \n",
      "Iter 109/1000 - Loss: 1.876 \n",
      "Iter 110/1000 - Loss: 1.872 \n",
      "Iter 111/1000 - Loss: 1.868 \n",
      "Iter 112/1000 - Loss: 1.863 \n",
      "Iter 113/1000 - Loss: 1.859 \n",
      "Iter 114/1000 - Loss: 1.855 \n",
      "Iter 115/1000 - Loss: 1.850 \n",
      "Iter 116/1000 - Loss: 1.846 \n",
      "Iter 117/1000 - Loss: 1.841 \n",
      "Iter 118/1000 - Loss: 1.837 \n",
      "Iter 119/1000 - Loss: 1.832 \n",
      "Iter 120/1000 - Loss: 1.827 \n",
      "Iter 121/1000 - Loss: 1.822 \n",
      "Iter 122/1000 - Loss: 1.818 \n",
      "Iter 123/1000 - Loss: 1.812 \n",
      "Iter 124/1000 - Loss: 1.807 \n",
      "Iter 125/1000 - Loss: 1.802 \n",
      "Iter 126/1000 - Loss: 1.797 \n",
      "Iter 127/1000 - Loss: 1.791 \n",
      "Iter 128/1000 - Loss: 1.786 \n",
      "Iter 129/1000 - Loss: 1.780 \n",
      "Iter 130/1000 - Loss: 1.774 \n",
      "Iter 131/1000 - Loss: 1.768 \n",
      "Iter 132/1000 - Loss: 1.761 \n",
      "Iter 133/1000 - Loss: 1.755 \n",
      "Iter 134/1000 - Loss: 1.748 \n",
      "Iter 135/1000 - Loss: 1.742 \n",
      "Iter 136/1000 - Loss: 1.735 \n",
      "Iter 137/1000 - Loss: 1.728 \n",
      "Iter 138/1000 - Loss: 1.720 \n",
      "Iter 139/1000 - Loss: 1.713 \n",
      "Iter 140/1000 - Loss: 1.705 \n",
      "Iter 141/1000 - Loss: 1.697 \n",
      "Iter 142/1000 - Loss: 1.689 \n",
      "Iter 143/1000 - Loss: 1.680 \n",
      "Iter 144/1000 - Loss: 1.672 \n",
      "Iter 145/1000 - Loss: 1.663 \n",
      "Iter 146/1000 - Loss: 1.653 \n",
      "Iter 147/1000 - Loss: 1.644 \n",
      "Iter 148/1000 - Loss: 1.634 \n",
      "Iter 149/1000 - Loss: 1.624 \n",
      "Iter 150/1000 - Loss: 1.614 \n",
      "Iter 151/1000 - Loss: 1.603 \n",
      "Iter 152/1000 - Loss: 1.592 \n",
      "Iter 153/1000 - Loss: 1.581 \n",
      "Iter 154/1000 - Loss: 1.570 \n",
      "Iter 155/1000 - Loss: 1.558 \n",
      "Iter 156/1000 - Loss: 1.546 \n",
      "Iter 157/1000 - Loss: 1.533 \n",
      "Iter 158/1000 - Loss: 1.521 \n",
      "Iter 159/1000 - Loss: 1.507 \n",
      "Iter 160/1000 - Loss: 1.494 \n",
      "Iter 161/1000 - Loss: 1.480 \n",
      "Iter 162/1000 - Loss: 1.466 \n",
      "Iter 163/1000 - Loss: 1.452 \n",
      "Iter 164/1000 - Loss: 1.437 \n",
      "Iter 165/1000 - Loss: 1.422 \n",
      "Iter 166/1000 - Loss: 1.407 \n",
      "Iter 167/1000 - Loss: 1.392 \n",
      "Iter 168/1000 - Loss: 1.376 \n",
      "Iter 169/1000 - Loss: 1.360 \n",
      "Iter 170/1000 - Loss: 1.343 \n",
      "Iter 171/1000 - Loss: 1.326 \n",
      "Iter 172/1000 - Loss: 1.309 \n",
      "Iter 173/1000 - Loss: 1.292 \n",
      "Iter 174/1000 - Loss: 1.274 \n",
      "Iter 175/1000 - Loss: 1.257 \n",
      "Iter 176/1000 - Loss: 1.239 \n",
      "Iter 177/1000 - Loss: 1.220 \n",
      "Iter 178/1000 - Loss: 1.202 \n",
      "Iter 179/1000 - Loss: 1.183 \n",
      "Iter 180/1000 - Loss: 1.164 \n",
      "Iter 181/1000 - Loss: 1.145 \n",
      "Iter 182/1000 - Loss: 1.126 \n",
      "Iter 183/1000 - Loss: 1.107 \n",
      "Iter 184/1000 - Loss: 1.087 \n",
      "Iter 185/1000 - Loss: 1.067 \n",
      "Iter 186/1000 - Loss: 1.048 \n",
      "Iter 187/1000 - Loss: 1.028 \n",
      "Iter 188/1000 - Loss: 1.008 \n",
      "Iter 189/1000 - Loss: 0.988 \n",
      "Iter 190/1000 - Loss: 0.968 \n",
      "Iter 191/1000 - Loss: 0.947 \n",
      "Iter 192/1000 - Loss: 0.927 \n",
      "Iter 193/1000 - Loss: 0.907 \n",
      "Iter 194/1000 - Loss: 0.886 \n",
      "Iter 195/1000 - Loss: 0.866 \n",
      "Iter 196/1000 - Loss: 0.845 \n",
      "Iter 197/1000 - Loss: 0.824 \n",
      "Iter 198/1000 - Loss: 0.804 \n",
      "Iter 199/1000 - Loss: 0.783 \n",
      "Iter 200/1000 - Loss: 0.762 \n",
      "Iter 201/1000 - Loss: 0.741 \n",
      "Iter 202/1000 - Loss: 0.721 \n",
      "Iter 203/1000 - Loss: 0.700 \n",
      "Iter 204/1000 - Loss: 0.679 \n",
      "Iter 205/1000 - Loss: 0.658 \n",
      "Iter 206/1000 - Loss: 0.637 \n",
      "Iter 207/1000 - Loss: 0.616 \n",
      "Iter 208/1000 - Loss: 0.596 \n",
      "Iter 209/1000 - Loss: 0.575 \n",
      "Iter 210/1000 - Loss: 0.554 \n",
      "Iter 211/1000 - Loss: 0.534 \n",
      "Iter 212/1000 - Loss: 0.516 \n",
      "Iter 213/1000 - Loss: 0.552 \n",
      "Iter 214/1000 - Loss: 0.582 \n",
      "Iter 215/1000 - Loss: 0.455 \n",
      "Iter 216/1000 - Loss: 0.542 \n",
      "Iter 217/1000 - Loss: 0.417 \n",
      "Iter 218/1000 - Loss: 0.499 \n",
      "Iter 219/1000 - Loss: 0.385 \n",
      "Iter 220/1000 - Loss: 0.452 \n",
      "Iter 221/1000 - Loss: 0.354 \n",
      "Iter 222/1000 - Loss: 0.405 \n",
      "Iter 223/1000 - Loss: 0.324 \n",
      "Iter 224/1000 - Loss: 0.361 \n",
      "Iter 225/1000 - Loss: 0.289 \n",
      "Iter 226/1000 - Loss: 0.325 \n",
      "Iter 227/1000 - Loss: 0.251 \n",
      "Iter 228/1000 - Loss: 0.294 \n",
      "Iter 229/1000 - Loss: 0.216 \n",
      "Iter 230/1000 - Loss: 0.259 \n",
      "Iter 231/1000 - Loss: 0.193 \n",
      "Iter 232/1000 - Loss: 0.206 \n",
      "Iter 233/1000 - Loss: 0.190 \n",
      "Iter 234/1000 - Loss: 0.144 \n",
      "Iter 235/1000 - Loss: 0.177 \n",
      "Iter 236/1000 - Loss: 0.128 \n",
      "Iter 237/1000 - Loss: 0.107 \n",
      "Iter 238/1000 - Loss: 0.133 \n",
      "Iter 239/1000 - Loss: 0.095 \n",
      "Iter 240/1000 - Loss: 0.057 \n",
      "Iter 241/1000 - Loss: 0.076 \n",
      "Iter 242/1000 - Loss: 0.089 \n",
      "Iter 243/1000 - Loss: 0.051 \n",
      "Iter 244/1000 - Loss: 0.009 \n",
      "Iter 245/1000 - Loss: 0.006 \n",
      "Iter 246/1000 - Loss: 0.033 \n",
      "Iter 247/1000 - Loss: 0.067 \n",
      "Iter 248/1000 - Loss: 0.062 \n",
      "Iter 249/1000 - Loss: -0.002 \n",
      "Iter 250/1000 - Loss: -0.054 \n",
      "Iter 251/1000 - Loss: -0.040 \n",
      "Iter 252/1000 - Loss: 0.009 \n",
      "Iter 253/1000 - Loss: 0.028 \n",
      "Iter 254/1000 - Loss: -0.025 \n",
      "Iter 255/1000 - Loss: -0.090 \n",
      "Iter 256/1000 - Loss: -0.085 \n",
      "Iter 257/1000 - Loss: -0.035 \n",
      "Iter 258/1000 - Loss: -0.021 \n",
      "Iter 259/1000 - Loss: -0.072 \n",
      "Iter 260/1000 - Loss: -0.124 \n",
      "Iter 261/1000 - Loss: -0.115 \n",
      "Iter 262/1000 - Loss: -0.074 \n",
      "Iter 263/1000 - Loss: -0.062 \n",
      "Iter 264/1000 - Loss: -0.103 \n",
      "Iter 265/1000 - Loss: -0.148 \n",
      "Iter 266/1000 - Loss: -0.147 \n",
      "Iter 267/1000 - Loss: -0.115 \n",
      "Iter 268/1000 - Loss: -0.098 \n",
      "Iter 269/1000 - Loss: -0.119 \n",
      "Iter 270/1000 - Loss: -0.160 \n",
      "Iter 271/1000 - Loss: -0.175 \n",
      "Iter 272/1000 - Loss: -0.159 \n",
      "Iter 273/1000 - Loss: -0.136 \n",
      "Iter 274/1000 - Loss: -0.131 \n",
      "Iter 275/1000 - Loss: -0.152 \n",
      "Iter 276/1000 - Loss: -0.181 \n",
      "Iter 277/1000 - Loss: -0.194 \n",
      "Iter 278/1000 - Loss: -0.187 \n",
      "Iter 279/1000 - Loss: -0.171 \n",
      "Iter 280/1000 - Loss: -0.159 \n",
      "Iter 281/1000 - Loss: -0.160 \n",
      "Iter 282/1000 - Loss: -0.177 \n",
      "Iter 283/1000 - Loss: -0.198 \n",
      "Iter 284/1000 - Loss: -0.210 \n",
      "Iter 285/1000 - Loss: -0.208 \n",
      "Iter 286/1000 - Loss: -0.199 \n",
      "Iter 287/1000 - Loss: -0.188 \n",
      "Iter 288/1000 - Loss: -0.182 \n",
      "Iter 289/1000 - Loss: -0.184 \n",
      "Iter 290/1000 - Loss: -0.194 \n",
      "Iter 291/1000 - Loss: -0.208 \n",
      "Iter 292/1000 - Loss: -0.220 \n",
      "Iter 293/1000 - Loss: -0.224 \n",
      "Iter 294/1000 - Loss: -0.222 \n",
      "Iter 295/1000 - Loss: -0.215 \n",
      "Iter 296/1000 - Loss: -0.208 \n",
      "Iter 297/1000 - Loss: -0.201 \n",
      "Iter 298/1000 - Loss: -0.198 \n",
      "Iter 299/1000 - Loss: -0.199 \n",
      "Iter 300/1000 - Loss: -0.205 \n",
      "Iter 301/1000 - Loss: -0.215 \n",
      "Iter 302/1000 - Loss: -0.226 \n",
      "Iter 303/1000 - Loss: -0.234 \n",
      "Iter 304/1000 - Loss: -0.236 \n",
      "Iter 305/1000 - Loss: -0.234 \n",
      "Iter 306/1000 - Loss: -0.229 \n",
      "Iter 307/1000 - Loss: -0.224 \n",
      "Iter 308/1000 - Loss: -0.219 \n",
      "Iter 309/1000 - Loss: -0.216 \n",
      "Iter 310/1000 - Loss: -0.215 \n",
      "Iter 311/1000 - Loss: -0.220 \n",
      "Iter 312/1000 - Loss: -0.227 \n",
      "Iter 313/1000 - Loss: -0.235 \n",
      "Iter 314/1000 - Loss: -0.240 \n",
      "Iter 315/1000 - Loss: -0.244 \n",
      "Iter 316/1000 - Loss: -0.245 \n",
      "Iter 317/1000 - Loss: -0.243 \n",
      "Iter 318/1000 - Loss: -0.240 \n",
      "Iter 319/1000 - Loss: -0.236 \n",
      "Iter 320/1000 - Loss: -0.234 \n",
      "Iter 321/1000 - Loss: -0.230 \n",
      "Iter 322/1000 - Loss: -0.227 \n",
      "Iter 323/1000 - Loss: -0.226 \n",
      "Iter 324/1000 - Loss: -0.227 \n",
      "Iter 325/1000 - Loss: -0.231 \n",
      "Iter 326/1000 - Loss: -0.237 \n",
      "Iter 327/1000 - Loss: -0.243 \n",
      "Iter 328/1000 - Loss: -0.248 \n",
      "Iter 329/1000 - Loss: -0.250 \n",
      "Iter 330/1000 - Loss: -0.252 \n",
      "Iter 331/1000 - Loss: -0.251 \n",
      "Iter 332/1000 - Loss: -0.249 \n",
      "Iter 333/1000 - Loss: -0.247 \n",
      "Iter 334/1000 - Loss: -0.243 \n",
      "Iter 335/1000 - Loss: -0.241 \n",
      "Iter 336/1000 - Loss: -0.238 \n",
      "Iter 337/1000 - Loss: -0.236 \n",
      "Iter 338/1000 - Loss: -0.234 \n",
      "Iter 339/1000 - Loss: -0.234 \n",
      "Iter 340/1000 - Loss: -0.236 \n",
      "Iter 341/1000 - Loss: -0.241 \n",
      "Iter 342/1000 - Loss: -0.246 \n",
      "Iter 343/1000 - Loss: -0.251 \n",
      "Iter 344/1000 - Loss: -0.255 \n",
      "Iter 345/1000 - Loss: -0.257 \n",
      "Iter 346/1000 - Loss: -0.257 \n",
      "Iter 347/1000 - Loss: -0.256 \n",
      "Iter 348/1000 - Loss: -0.255 \n",
      "Iter 349/1000 - Loss: -0.252 \n",
      "Iter 350/1000 - Loss: -0.251 \n",
      "Iter 351/1000 - Loss: -0.249 \n",
      "Iter 352/1000 - Loss: -0.247 \n",
      "Iter 353/1000 - Loss: -0.245 \n",
      "Iter 354/1000 - Loss: -0.243 \n",
      "Iter 355/1000 - Loss: -0.242 \n",
      "Iter 356/1000 - Loss: -0.244 \n",
      "Iter 357/1000 - Loss: -0.245 \n",
      "Iter 358/1000 - Loss: -0.248 \n",
      "Iter 359/1000 - Loss: -0.252 \n",
      "Iter 360/1000 - Loss: -0.255 \n",
      "Iter 361/1000 - Loss: -0.258 \n",
      "Iter 362/1000 - Loss: -0.260 \n",
      "Iter 363/1000 - Loss: -0.261 \n",
      "Iter 364/1000 - Loss: -0.260 \n",
      "Iter 365/1000 - Loss: -0.261 \n",
      "Iter 366/1000 - Loss: -0.259 \n",
      "Iter 367/1000 - Loss: -0.258 \n",
      "Iter 368/1000 - Loss: -0.257 \n",
      "Iter 369/1000 - Loss: -0.255 \n",
      "Iter 370/1000 - Loss: -0.254 \n",
      "Iter 371/1000 - Loss: -0.253 \n",
      "Iter 372/1000 - Loss: -0.251 \n",
      "Iter 373/1000 - Loss: -0.248 \n",
      "Iter 374/1000 - Loss: -0.245 \n",
      "Iter 375/1000 - Loss: -0.242 \n",
      "Iter 376/1000 - Loss: -0.241 \n",
      "Iter 377/1000 - Loss: -0.243 \n",
      "Iter 378/1000 - Loss: -0.244 \n",
      "Iter 379/1000 - Loss: -0.248 \n",
      "Iter 380/1000 - Loss: -0.252 \n",
      "Iter 381/1000 - Loss: -0.257 \n",
      "Iter 382/1000 - Loss: -0.261 \n",
      "Iter 383/1000 - Loss: -0.262 \n",
      "Iter 384/1000 - Loss: -0.263 \n",
      "Iter 385/1000 - Loss: -0.264 \n",
      "Iter 386/1000 - Loss: -0.263 \n",
      "Iter 387/1000 - Loss: -0.261 \n",
      "Iter 388/1000 - Loss: -0.260 \n",
      "Iter 389/1000 - Loss: -0.258 \n",
      "Iter 390/1000 - Loss: -0.258 \n",
      "Iter 391/1000 - Loss: -0.257 \n",
      "Iter 392/1000 - Loss: -0.257 \n",
      "Iter 393/1000 - Loss: -0.258 \n",
      "Iter 394/1000 - Loss: -0.259 \n",
      "Iter 395/1000 - Loss: -0.260 \n",
      "Iter 396/1000 - Loss: -0.261 \n",
      "Iter 397/1000 - Loss: -0.261 \n",
      "Iter 398/1000 - Loss: -0.262 \n",
      "Iter 399/1000 - Loss: -0.263 \n",
      "Iter 400/1000 - Loss: -0.264 \n",
      "Iter 401/1000 - Loss: -0.264 \n",
      "Iter 402/1000 - Loss: -0.266 \n",
      "Iter 403/1000 - Loss: -0.267 \n",
      "Iter 404/1000 - Loss: -0.266 \n",
      "Iter 405/1000 - Loss: -0.266 \n",
      "Iter 406/1000 - Loss: -0.267 \n",
      "Iter 407/1000 - Loss: -0.267 \n",
      "Iter 408/1000 - Loss: -0.267 \n",
      "Iter 409/1000 - Loss: -0.267 \n",
      "Iter 410/1000 - Loss: -0.267 \n",
      "Iter 411/1000 - Loss: -0.266 \n",
      "Iter 412/1000 - Loss: -0.267 \n",
      "Iter 413/1000 - Loss: -0.267 \n",
      "Iter 414/1000 - Loss: -0.267 \n",
      "Iter 415/1000 - Loss: -0.267 \n",
      "Iter 416/1000 - Loss: -0.267 \n",
      "Iter 417/1000 - Loss: -0.266 \n",
      "Iter 418/1000 - Loss: -0.265 \n",
      "Iter 419/1000 - Loss: -0.263 \n",
      "Iter 420/1000 - Loss: -0.259 \n",
      "Iter 421/1000 - Loss: -0.252 \n",
      "Iter 422/1000 - Loss: -0.241 \n",
      "Iter 423/1000 - Loss: -0.221 \n",
      "Iter 424/1000 - Loss: -0.189 \n",
      "Iter 425/1000 - Loss: -0.151 \n",
      "Iter 426/1000 - Loss: -0.126 \n",
      "Iter 427/1000 - Loss: -0.149 \n",
      "Iter 428/1000 - Loss: -0.218 \n",
      "Iter 429/1000 - Loss: -0.267 \n",
      "Iter 430/1000 - Loss: -0.251 \n",
      "Iter 431/1000 - Loss: -0.208 \n",
      "Iter 432/1000 - Loss: -0.201 \n",
      "Iter 433/1000 - Loss: -0.239 \n",
      "Iter 434/1000 - Loss: -0.268 \n",
      "Iter 435/1000 - Loss: -0.253 \n",
      "Iter 436/1000 - Loss: -0.229 \n",
      "Iter 437/1000 - Loss: -0.237 \n",
      "Iter 438/1000 - Loss: -0.263 \n",
      "Iter 439/1000 - Loss: -0.266 \n",
      "Iter 440/1000 - Loss: -0.248 \n",
      "Iter 441/1000 - Loss: -0.244 \n",
      "Iter 442/1000 - Loss: -0.261 \n",
      "Iter 443/1000 - Loss: -0.270 \n",
      "Iter 444/1000 - Loss: -0.262 \n",
      "Iter 445/1000 - Loss: -0.253 \n",
      "Iter 446/1000 - Loss: -0.259 \n",
      "Iter 447/1000 - Loss: -0.269 \n",
      "Iter 448/1000 - Loss: -0.267 \n",
      "Iter 449/1000 - Loss: -0.260 \n",
      "Iter 450/1000 - Loss: -0.260 \n",
      "Iter 451/1000 - Loss: -0.267 \n",
      "Iter 452/1000 - Loss: -0.270 \n",
      "Iter 453/1000 - Loss: -0.265 \n",
      "Iter 454/1000 - Loss: -0.262 \n",
      "Iter 455/1000 - Loss: -0.266 \n",
      "Iter 456/1000 - Loss: -0.270 \n",
      "Iter 457/1000 - Loss: -0.268 \n",
      "Iter 458/1000 - Loss: -0.265 \n",
      "Iter 459/1000 - Loss: -0.266 \n",
      "Iter 460/1000 - Loss: -0.270 \n",
      "Iter 461/1000 - Loss: -0.270 \n",
      "Iter 462/1000 - Loss: -0.268 \n",
      "Iter 463/1000 - Loss: -0.268 \n",
      "Iter 464/1000 - Loss: -0.269 \n",
      "Iter 465/1000 - Loss: -0.270 \n",
      "Iter 466/1000 - Loss: -0.270 \n",
      "Iter 467/1000 - Loss: -0.269 \n",
      "Iter 468/1000 - Loss: -0.269 \n",
      "Iter 469/1000 - Loss: -0.270 \n",
      "Iter 470/1000 - Loss: -0.271 \n",
      "Iter 471/1000 - Loss: -0.270 \n",
      "Iter 472/1000 - Loss: -0.270 \n",
      "Iter 473/1000 - Loss: -0.270 \n",
      "Iter 474/1000 - Loss: -0.271 \n",
      "Iter 475/1000 - Loss: -0.271 \n",
      "Iter 476/1000 - Loss: -0.270 \n",
      "Iter 477/1000 - Loss: -0.270 \n",
      "Iter 478/1000 - Loss: -0.270 \n",
      "Iter 479/1000 - Loss: -0.271 \n",
      "Iter 480/1000 - Loss: -0.271 \n",
      "Iter 481/1000 - Loss: -0.271 \n",
      "Iter 482/1000 - Loss: -0.272 \n",
      "Iter 483/1000 - Loss: -0.271 \n",
      "Iter 484/1000 - Loss: -0.272 \n",
      "Iter 485/1000 - Loss: -0.272 \n",
      "Iter 486/1000 - Loss: -0.271 \n",
      "Iter 487/1000 - Loss: -0.272 \n",
      "Iter 488/1000 - Loss: -0.271 \n",
      "Iter 489/1000 - Loss: -0.271 \n",
      "Iter 490/1000 - Loss: -0.271 \n",
      "Iter 491/1000 - Loss: -0.272 \n",
      "Iter 492/1000 - Loss: -0.272 \n",
      "Iter 493/1000 - Loss: -0.272 \n",
      "Iter 494/1000 - Loss: -0.272 \n",
      "Iter 495/1000 - Loss: -0.271 \n",
      "Iter 496/1000 - Loss: -0.272 \n",
      "Iter 497/1000 - Loss: -0.272 \n",
      "Iter 498/1000 - Loss: -0.272 \n",
      "Iter 499/1000 - Loss: -0.272 \n",
      "Iter 500/1000 - Loss: -0.272 \n",
      "Iter 501/1000 - Loss: -0.272 \n",
      "Iter 502/1000 - Loss: -0.272 \n",
      "Iter 503/1000 - Loss: -0.272 \n",
      "Iter 504/1000 - Loss: -0.272 \n",
      "Iter 505/1000 - Loss: -0.272 \n",
      "Iter 506/1000 - Loss: -0.271 \n",
      "Iter 507/1000 - Loss: -0.273 \n",
      "Iter 508/1000 - Loss: -0.273 \n",
      "Iter 509/1000 - Loss: -0.273 \n",
      "Iter 510/1000 - Loss: -0.273 \n",
      "Iter 511/1000 - Loss: -0.273 \n",
      "Iter 512/1000 - Loss: -0.273 \n",
      "Iter 513/1000 - Loss: -0.273 \n",
      "Iter 514/1000 - Loss: -0.273 \n",
      "Iter 515/1000 - Loss: -0.273 \n",
      "Iter 516/1000 - Loss: -0.273 \n",
      "Iter 517/1000 - Loss: -0.273 \n",
      "Iter 518/1000 - Loss: -0.273 \n",
      "Iter 519/1000 - Loss: -0.273 \n",
      "Iter 520/1000 - Loss: -0.273 \n",
      "Iter 521/1000 - Loss: -0.273 \n",
      "Iter 522/1000 - Loss: -0.273 \n",
      "Iter 523/1000 - Loss: -0.273 \n",
      "Iter 524/1000 - Loss: -0.273 \n",
      "Iter 525/1000 - Loss: -0.273 \n",
      "Iter 526/1000 - Loss: -0.273 \n",
      "Iter 527/1000 - Loss: -0.273 \n",
      "Iter 528/1000 - Loss: -0.273 \n",
      "Iter 529/1000 - Loss: -0.273 \n",
      "Iter 530/1000 - Loss: -0.273 \n",
      "Iter 531/1000 - Loss: -0.273 \n",
      "Iter 532/1000 - Loss: -0.273 \n",
      "Iter 533/1000 - Loss: -0.273 \n",
      "Iter 534/1000 - Loss: -0.273 \n",
      "Iter 535/1000 - Loss: -0.273 \n",
      "Iter 536/1000 - Loss: -0.273 \n",
      "Iter 537/1000 - Loss: -0.273 \n",
      "Iter 538/1000 - Loss: -0.273 \n",
      "Iter 539/1000 - Loss: -0.273 \n",
      "Iter 540/1000 - Loss: -0.273 \n",
      "Iter 541/1000 - Loss: -0.274 \n",
      "Iter 542/1000 - Loss: -0.274 \n",
      "Iter 543/1000 - Loss: -0.274 \n",
      "Iter 544/1000 - Loss: -0.274 \n",
      "Iter 545/1000 - Loss: -0.274 \n",
      "Iter 546/1000 - Loss: -0.274 \n",
      "Iter 547/1000 - Loss: -0.274 \n",
      "Iter 548/1000 - Loss: -0.274 \n",
      "Iter 549/1000 - Loss: -0.274 \n",
      "Iter 550/1000 - Loss: -0.274 \n",
      "Iter 551/1000 - Loss: -0.274 \n",
      "Iter 552/1000 - Loss: -0.274 \n",
      "Iter 553/1000 - Loss: -0.274 \n",
      "Iter 554/1000 - Loss: -0.273 \n",
      "Iter 555/1000 - Loss: -0.273 \n",
      "Iter 556/1000 - Loss: -0.274 \n",
      "Iter 557/1000 - Loss: -0.273 \n",
      "Iter 558/1000 - Loss: -0.273 \n",
      "Iter 559/1000 - Loss: -0.275 \n",
      "Iter 560/1000 - Loss: -0.274 \n",
      "Iter 561/1000 - Loss: -0.274 \n",
      "Iter 562/1000 - Loss: -0.274 \n",
      "Iter 563/1000 - Loss: -0.273 \n",
      "Iter 564/1000 - Loss: -0.273 \n",
      "Iter 565/1000 - Loss: -0.274 \n",
      "Iter 566/1000 - Loss: -0.273 \n",
      "Iter 567/1000 - Loss: -0.272 \n",
      "Iter 568/1000 - Loss: -0.272 \n",
      "Iter 569/1000 - Loss: -0.269 \n",
      "Iter 570/1000 - Loss: -0.266 \n",
      "Iter 571/1000 - Loss: -0.261 \n",
      "Iter 572/1000 - Loss: -0.251 \n",
      "Iter 573/1000 - Loss: -0.235 \n",
      "Iter 574/1000 - Loss: -0.211 \n",
      "Iter 575/1000 - Loss: -0.181 \n",
      "Iter 576/1000 - Loss: -0.155 \n",
      "Iter 577/1000 - Loss: -0.154 \n",
      "Iter 578/1000 - Loss: -0.189 \n",
      "Iter 579/1000 - Loss: -0.243 \n",
      "Iter 580/1000 - Loss: -0.273 \n",
      "Iter 581/1000 - Loss: -0.261 \n",
      "Iter 582/1000 - Loss: -0.231 \n",
      "Iter 583/1000 - Loss: -0.219 \n",
      "Iter 584/1000 - Loss: -0.237 \n",
      "Iter 585/1000 - Loss: -0.265 \n",
      "Iter 586/1000 - Loss: -0.273 \n",
      "Iter 587/1000 - Loss: -0.259 \n",
      "Iter 588/1000 - Loss: -0.244 \n",
      "Iter 589/1000 - Loss: -0.251 \n",
      "Iter 590/1000 - Loss: -0.267 \n",
      "Iter 591/1000 - Loss: -0.275 \n",
      "Iter 592/1000 - Loss: -0.269 \n",
      "Iter 593/1000 - Loss: -0.259 \n",
      "Iter 594/1000 - Loss: -0.259 \n",
      "Iter 595/1000 - Loss: -0.269 \n",
      "Iter 596/1000 - Loss: -0.275 \n",
      "Iter 597/1000 - Loss: -0.272 \n",
      "Iter 598/1000 - Loss: -0.266 \n",
      "Iter 599/1000 - Loss: -0.265 \n",
      "Iter 600/1000 - Loss: -0.270 \n",
      "Iter 601/1000 - Loss: -0.275 \n",
      "Iter 602/1000 - Loss: -0.274 \n",
      "Iter 603/1000 - Loss: -0.270 \n",
      "Iter 604/1000 - Loss: -0.269 \n",
      "Iter 605/1000 - Loss: -0.271 \n",
      "Iter 606/1000 - Loss: -0.274 \n",
      "Iter 607/1000 - Loss: -0.274 \n",
      "Iter 608/1000 - Loss: -0.273 \n",
      "Iter 609/1000 - Loss: -0.271 \n",
      "Iter 610/1000 - Loss: -0.272 \n",
      "Iter 611/1000 - Loss: -0.275 \n",
      "Iter 612/1000 - Loss: -0.275 \n",
      "Iter 613/1000 - Loss: -0.274 \n",
      "Iter 614/1000 - Loss: -0.274 \n",
      "Iter 615/1000 - Loss: -0.273 \n",
      "Iter 616/1000 - Loss: -0.274 \n",
      "Iter 617/1000 - Loss: -0.275 \n",
      "Iter 618/1000 - Loss: -0.275 \n",
      "Iter 619/1000 - Loss: -0.275 \n",
      "Iter 620/1000 - Loss: -0.274 \n",
      "Iter 621/1000 - Loss: -0.274 \n",
      "Iter 622/1000 - Loss: -0.275 \n",
      "Iter 623/1000 - Loss: -0.275 \n",
      "Iter 624/1000 - Loss: -0.275 \n",
      "Iter 625/1000 - Loss: -0.274 \n",
      "Iter 626/1000 - Loss: -0.275 \n",
      "Iter 627/1000 - Loss: -0.274 \n",
      "Iter 628/1000 - Loss: -0.275 \n",
      "Iter 629/1000 - Loss: -0.275 \n",
      "Iter 630/1000 - Loss: -0.275 \n",
      "Iter 631/1000 - Loss: -0.274 \n",
      "Iter 632/1000 - Loss: -0.275 \n",
      "Iter 633/1000 - Loss: -0.274 \n",
      "Iter 634/1000 - Loss: -0.275 \n",
      "Iter 635/1000 - Loss: -0.275 \n",
      "Iter 636/1000 - Loss: -0.276 \n",
      "Iter 637/1000 - Loss: -0.275 \n",
      "Iter 638/1000 - Loss: -0.275 \n",
      "Iter 639/1000 - Loss: -0.274 \n",
      "Iter 640/1000 - Loss: -0.275 \n",
      "Iter 641/1000 - Loss: -0.275 \n",
      "Iter 642/1000 - Loss: -0.276 \n",
      "Iter 643/1000 - Loss: -0.275 \n",
      "Iter 644/1000 - Loss: -0.275 \n",
      "Iter 645/1000 - Loss: -0.275 \n",
      "Iter 646/1000 - Loss: -0.275 \n",
      "Iter 647/1000 - Loss: -0.275 \n",
      "Iter 648/1000 - Loss: -0.275 \n",
      "Iter 649/1000 - Loss: -0.275 \n",
      "Iter 650/1000 - Loss: -0.275 \n",
      "Iter 651/1000 - Loss: -0.275 \n",
      "Iter 652/1000 - Loss: -0.275 \n",
      "Iter 653/1000 - Loss: -0.275 \n",
      "Iter 654/1000 - Loss: -0.275 \n",
      "Iter 655/1000 - Loss: -0.275 \n",
      "Iter 656/1000 - Loss: -0.275 \n",
      "Iter 657/1000 - Loss: -0.276 \n",
      "Iter 658/1000 - Loss: -0.276 \n",
      "Iter 659/1000 - Loss: -0.276 \n",
      "Iter 660/1000 - Loss: -0.276 \n",
      "Iter 661/1000 - Loss: -0.276 \n",
      "Iter 662/1000 - Loss: -0.276 \n",
      "Iter 663/1000 - Loss: -0.276 \n",
      "Iter 664/1000 - Loss: -0.276 \n",
      "Iter 665/1000 - Loss: -0.276 \n",
      "Iter 666/1000 - Loss: -0.276 \n",
      "Iter 667/1000 - Loss: -0.276 \n",
      "Iter 668/1000 - Loss: -0.276 \n",
      "Iter 669/1000 - Loss: -0.276 \n",
      "Iter 670/1000 - Loss: -0.276 \n",
      "Iter 671/1000 - Loss: -0.275 \n",
      "Iter 672/1000 - Loss: -0.276 \n",
      "Iter 673/1000 - Loss: -0.277 \n",
      "Iter 674/1000 - Loss: -0.276 \n",
      "Iter 675/1000 - Loss: -0.276 \n",
      "Iter 676/1000 - Loss: -0.276 \n",
      "Iter 677/1000 - Loss: -0.276 \n",
      "Iter 678/1000 - Loss: -0.276 \n",
      "Iter 679/1000 - Loss: -0.276 \n",
      "Iter 680/1000 - Loss: -0.275 \n",
      "Iter 681/1000 - Loss: -0.276 \n",
      "Iter 682/1000 - Loss: -0.276 \n",
      "Iter 683/1000 - Loss: -0.276 \n",
      "Iter 684/1000 - Loss: -0.276 \n",
      "Iter 685/1000 - Loss: -0.276 \n",
      "Iter 686/1000 - Loss: -0.276 \n",
      "Iter 687/1000 - Loss: -0.276 \n",
      "Iter 688/1000 - Loss: -0.276 \n",
      "Iter 689/1000 - Loss: -0.276 \n",
      "Iter 690/1000 - Loss: -0.276 \n",
      "Iter 691/1000 - Loss: -0.276 \n",
      "Iter 692/1000 - Loss: -0.276 \n",
      "Iter 693/1000 - Loss: -0.276 \n",
      "Iter 694/1000 - Loss: -0.276 \n",
      "Iter 695/1000 - Loss: -0.276 \n",
      "Iter 696/1000 - Loss: -0.276 \n",
      "Iter 697/1000 - Loss: -0.276 \n",
      "Iter 698/1000 - Loss: -0.276 \n",
      "Iter 699/1000 - Loss: -0.276 \n",
      "Iter 700/1000 - Loss: -0.276 \n",
      "Iter 701/1000 - Loss: -0.276 \n",
      "Iter 702/1000 - Loss: -0.276 \n",
      "Iter 703/1000 - Loss: -0.276 \n",
      "Iter 704/1000 - Loss: -0.276 \n",
      "Iter 705/1000 - Loss: -0.276 \n",
      "Iter 706/1000 - Loss: -0.276 \n",
      "Iter 707/1000 - Loss: -0.276 \n",
      "Iter 708/1000 - Loss: -0.276 \n",
      "Iter 709/1000 - Loss: -0.276 \n",
      "Iter 710/1000 - Loss: -0.276 \n",
      "Iter 711/1000 - Loss: -0.276 \n",
      "Iter 712/1000 - Loss: -0.275 \n",
      "Iter 713/1000 - Loss: -0.275 \n",
      "Iter 714/1000 - Loss: -0.274 \n",
      "Iter 715/1000 - Loss: -0.272 \n",
      "Iter 716/1000 - Loss: -0.269 \n",
      "Iter 717/1000 - Loss: -0.264 \n",
      "Iter 718/1000 - Loss: -0.256 \n",
      "Iter 719/1000 - Loss: -0.243 \n",
      "Iter 720/1000 - Loss: -0.222 \n",
      "Iter 721/1000 - Loss: -0.195 \n",
      "Iter 722/1000 - Loss: -0.162 \n",
      "Iter 723/1000 - Loss: -0.145 \n",
      "Iter 724/1000 - Loss: -0.162 \n",
      "Iter 725/1000 - Loss: -0.209 \n",
      "Iter 726/1000 - Loss: -0.258 \n",
      "Iter 727/1000 - Loss: -0.276 \n",
      "Iter 728/1000 - Loss: -0.255 \n",
      "Iter 729/1000 - Loss: -0.226 \n",
      "Iter 730/1000 - Loss: -0.222 \n",
      "Iter 731/1000 - Loss: -0.246 \n",
      "Iter 732/1000 - Loss: -0.272 \n",
      "Iter 733/1000 - Loss: -0.274 \n",
      "Iter 734/1000 - Loss: -0.256 \n",
      "Iter 735/1000 - Loss: -0.244 \n",
      "Iter 736/1000 - Loss: -0.252 \n",
      "Iter 737/1000 - Loss: -0.269 \n",
      "Iter 738/1000 - Loss: -0.276 \n",
      "Iter 739/1000 - Loss: -0.268 \n",
      "Iter 740/1000 - Loss: -0.259 \n",
      "Iter 741/1000 - Loss: -0.260 \n",
      "Iter 742/1000 - Loss: -0.270 \n",
      "Iter 743/1000 - Loss: -0.276 \n",
      "Iter 744/1000 - Loss: -0.274 \n",
      "Iter 745/1000 - Loss: -0.267 \n",
      "Iter 746/1000 - Loss: -0.266 \n",
      "Iter 747/1000 - Loss: -0.271 \n",
      "Iter 748/1000 - Loss: -0.276 \n",
      "Iter 749/1000 - Loss: -0.275 \n",
      "Iter 750/1000 - Loss: -0.272 \n",
      "Iter 751/1000 - Loss: -0.270 \n",
      "Iter 752/1000 - Loss: -0.272 \n",
      "Iter 753/1000 - Loss: -0.275 \n",
      "Iter 754/1000 - Loss: -0.276 \n",
      "Iter 755/1000 - Loss: -0.274 \n",
      "Iter 756/1000 - Loss: -0.273 \n",
      "Iter 757/1000 - Loss: -0.273 \n",
      "Iter 758/1000 - Loss: -0.275 \n",
      "Iter 759/1000 - Loss: -0.276 \n",
      "Iter 760/1000 - Loss: -0.276 \n",
      "Iter 761/1000 - Loss: -0.274 \n",
      "Iter 762/1000 - Loss: -0.274 \n",
      "Iter 763/1000 - Loss: -0.274 \n",
      "Iter 764/1000 - Loss: -0.276 \n",
      "Iter 765/1000 - Loss: -0.277 \n",
      "Iter 766/1000 - Loss: -0.276 \n",
      "Iter 767/1000 - Loss: -0.276 \n",
      "Iter 768/1000 - Loss: -0.275 \n",
      "Iter 769/1000 - Loss: -0.276 \n",
      "Iter 770/1000 - Loss: -0.277 \n",
      "Iter 771/1000 - Loss: -0.277 \n",
      "Iter 772/1000 - Loss: -0.277 \n",
      "Iter 773/1000 - Loss: -0.276 \n",
      "Iter 774/1000 - Loss: -0.276 \n",
      "Iter 775/1000 - Loss: -0.277 \n",
      "Iter 776/1000 - Loss: -0.277 \n",
      "Iter 777/1000 - Loss: -0.277 \n",
      "Iter 778/1000 - Loss: -0.277 \n",
      "Iter 779/1000 - Loss: -0.277 \n",
      "Iter 780/1000 - Loss: -0.276 \n",
      "Iter 781/1000 - Loss: -0.277 \n",
      "Iter 782/1000 - Loss: -0.277 \n",
      "Iter 783/1000 - Loss: -0.277 \n",
      "Iter 784/1000 - Loss: -0.277 \n",
      "Iter 785/1000 - Loss: -0.277 \n",
      "Iter 786/1000 - Loss: -0.277 \n",
      "Iter 787/1000 - Loss: -0.277 \n",
      "Iter 788/1000 - Loss: -0.277 \n",
      "Iter 789/1000 - Loss: -0.278 \n",
      "Iter 790/1000 - Loss: -0.277 \n",
      "Iter 791/1000 - Loss: -0.277 \n",
      "Iter 792/1000 - Loss: -0.277 \n",
      "Iter 793/1000 - Loss: -0.277 \n",
      "Iter 794/1000 - Loss: -0.277 \n",
      "Iter 795/1000 - Loss: -0.277 \n",
      "Iter 796/1000 - Loss: -0.277 \n",
      "Iter 797/1000 - Loss: -0.277 \n",
      "Iter 798/1000 - Loss: -0.277 \n",
      "Iter 799/1000 - Loss: -0.277 \n",
      "Iter 800/1000 - Loss: -0.277 \n",
      "Iter 801/1000 - Loss: -0.277 \n",
      "Iter 802/1000 - Loss: -0.277 \n",
      "Iter 803/1000 - Loss: -0.277 \n",
      "Iter 804/1000 - Loss: -0.277 \n",
      "Iter 805/1000 - Loss: -0.277 \n",
      "Iter 806/1000 - Loss: -0.277 \n",
      "Iter 807/1000 - Loss: -0.277 \n",
      "Iter 808/1000 - Loss: -0.277 \n",
      "Iter 809/1000 - Loss: -0.277 \n",
      "Iter 810/1000 - Loss: -0.277 \n",
      "Iter 811/1000 - Loss: -0.277 \n",
      "Iter 812/1000 - Loss: -0.277 \n",
      "Iter 813/1000 - Loss: -0.277 \n",
      "Iter 814/1000 - Loss: -0.277 \n",
      "Iter 815/1000 - Loss: -0.277 \n",
      "Iter 816/1000 - Loss: -0.277 \n",
      "Iter 817/1000 - Loss: -0.277 \n",
      "Iter 818/1000 - Loss: -0.277 \n",
      "Iter 819/1000 - Loss: -0.277 \n",
      "Iter 820/1000 - Loss: -0.277 \n",
      "Iter 821/1000 - Loss: -0.277 \n",
      "Iter 822/1000 - Loss: -0.277 \n",
      "Iter 823/1000 - Loss: -0.277 \n",
      "Iter 824/1000 - Loss: -0.277 \n",
      "Iter 825/1000 - Loss: -0.277 \n",
      "Iter 826/1000 - Loss: -0.277 \n",
      "Iter 827/1000 - Loss: -0.277 \n",
      "Iter 828/1000 - Loss: -0.277 \n",
      "Iter 829/1000 - Loss: -0.277 \n",
      "Iter 830/1000 - Loss: -0.278 \n",
      "Iter 831/1000 - Loss: -0.277 \n",
      "Iter 832/1000 - Loss: -0.277 \n",
      "Iter 833/1000 - Loss: -0.277 \n",
      "Iter 834/1000 - Loss: -0.277 \n",
      "Iter 835/1000 - Loss: -0.277 \n",
      "Iter 836/1000 - Loss: -0.277 \n",
      "Iter 837/1000 - Loss: -0.277 \n",
      "Iter 838/1000 - Loss: -0.277 \n",
      "Iter 839/1000 - Loss: -0.277 \n",
      "Iter 840/1000 - Loss: -0.277 \n",
      "Iter 841/1000 - Loss: -0.277 \n",
      "Iter 842/1000 - Loss: -0.277 \n",
      "Iter 843/1000 - Loss: -0.277 \n",
      "Iter 844/1000 - Loss: -0.276 \n",
      "Iter 845/1000 - Loss: -0.277 \n",
      "Iter 846/1000 - Loss: -0.278 \n",
      "Iter 847/1000 - Loss: -0.277 \n",
      "Iter 848/1000 - Loss: -0.277 \n",
      "Iter 849/1000 - Loss: -0.277 \n",
      "Iter 850/1000 - Loss: -0.278 \n",
      "Iter 851/1000 - Loss: -0.277 \n",
      "Iter 852/1000 - Loss: -0.277 \n",
      "Iter 853/1000 - Loss: -0.277 \n",
      "Iter 854/1000 - Loss: -0.277 \n",
      "Iter 855/1000 - Loss: -0.277 \n",
      "Iter 856/1000 - Loss: -0.278 \n",
      "Iter 857/1000 - Loss: -0.277 \n",
      "Iter 858/1000 - Loss: -0.277 \n",
      "Iter 859/1000 - Loss: -0.277 \n",
      "Iter 860/1000 - Loss: -0.277 \n",
      "Iter 861/1000 - Loss: -0.277 \n",
      "Iter 862/1000 - Loss: -0.277 \n",
      "Iter 863/1000 - Loss: -0.277 \n",
      "Iter 864/1000 - Loss: -0.277 \n",
      "Iter 865/1000 - Loss: -0.277 \n",
      "Iter 866/1000 - Loss: -0.278 \n",
      "Iter 867/1000 - Loss: -0.278 \n",
      "Iter 868/1000 - Loss: -0.277 \n",
      "Iter 869/1000 - Loss: -0.277 \n",
      "Iter 870/1000 - Loss: -0.277 \n",
      "Iter 871/1000 - Loss: -0.277 \n",
      "Iter 872/1000 - Loss: -0.277 \n",
      "Iter 873/1000 - Loss: -0.276 \n",
      "Iter 874/1000 - Loss: -0.276 \n",
      "Iter 875/1000 - Loss: -0.276 \n",
      "Iter 876/1000 - Loss: -0.276 \n",
      "Iter 877/1000 - Loss: -0.274 \n",
      "Iter 878/1000 - Loss: -0.272 \n",
      "Iter 879/1000 - Loss: -0.268 \n",
      "Iter 880/1000 - Loss: -0.260 \n",
      "Iter 881/1000 - Loss: -0.250 \n",
      "Iter 882/1000 - Loss: -0.233 \n",
      "Iter 883/1000 - Loss: -0.209 \n",
      "Iter 884/1000 - Loss: -0.180 \n",
      "Iter 885/1000 - Loss: -0.153 \n",
      "Iter 886/1000 - Loss: -0.142 \n",
      "Iter 887/1000 - Loss: -0.167 \n",
      "Iter 888/1000 - Loss: -0.221 \n",
      "Iter 889/1000 - Loss: -0.267 \n",
      "Iter 890/1000 - Loss: -0.275 \n",
      "Iter 891/1000 - Loss: -0.249 \n",
      "Iter 892/1000 - Loss: -0.220 \n",
      "Iter 893/1000 - Loss: -0.217 \n",
      "Iter 894/1000 - Loss: -0.243 \n",
      "Iter 895/1000 - Loss: -0.271 \n",
      "Iter 896/1000 - Loss: -0.275 \n",
      "Iter 897/1000 - Loss: -0.259 \n",
      "Iter 898/1000 - Loss: -0.244 \n",
      "Iter 899/1000 - Loss: -0.250 \n",
      "Iter 900/1000 - Loss: -0.267 \n",
      "Iter 901/1000 - Loss: -0.277 \n",
      "Iter 902/1000 - Loss: -0.272 \n",
      "Iter 903/1000 - Loss: -0.262 \n",
      "Iter 904/1000 - Loss: -0.260 \n",
      "Iter 905/1000 - Loss: -0.267 \n",
      "Iter 906/1000 - Loss: -0.276 \n",
      "Iter 907/1000 - Loss: -0.276 \n",
      "Iter 908/1000 - Loss: -0.270 \n",
      "Iter 909/1000 - Loss: -0.266 \n",
      "Iter 910/1000 - Loss: -0.269 \n",
      "Iter 911/1000 - Loss: -0.274 \n",
      "Iter 912/1000 - Loss: -0.277 \n",
      "Iter 913/1000 - Loss: -0.275 \n",
      "Iter 914/1000 - Loss: -0.271 \n",
      "Iter 915/1000 - Loss: -0.271 \n",
      "Iter 916/1000 - Loss: -0.275 \n",
      "Iter 917/1000 - Loss: -0.278 \n",
      "Iter 918/1000 - Loss: -0.276 \n",
      "Iter 919/1000 - Loss: -0.275 \n",
      "Iter 920/1000 - Loss: -0.274 \n",
      "Iter 921/1000 - Loss: -0.275 \n",
      "Iter 922/1000 - Loss: -0.276 \n",
      "Iter 923/1000 - Loss: -0.277 \n",
      "Iter 924/1000 - Loss: -0.276 \n",
      "Iter 925/1000 - Loss: -0.275 \n",
      "Iter 926/1000 - Loss: -0.275 \n",
      "Iter 927/1000 - Loss: -0.276 \n",
      "Iter 928/1000 - Loss: -0.277 \n",
      "Iter 929/1000 - Loss: -0.277 \n",
      "Iter 930/1000 - Loss: -0.276 \n",
      "Iter 931/1000 - Loss: -0.276 \n",
      "Iter 932/1000 - Loss: -0.276 \n",
      "Iter 933/1000 - Loss: -0.277 \n",
      "Iter 934/1000 - Loss: -0.277 \n",
      "Iter 935/1000 - Loss: -0.277 \n",
      "Iter 936/1000 - Loss: -0.277 \n",
      "Iter 937/1000 - Loss: -0.276 \n",
      "Iter 938/1000 - Loss: -0.276 \n",
      "Iter 939/1000 - Loss: -0.277 \n",
      "Iter 940/1000 - Loss: -0.277 \n",
      "Iter 941/1000 - Loss: -0.277 \n",
      "Iter 942/1000 - Loss: -0.278 \n",
      "Iter 943/1000 - Loss: -0.277 \n",
      "Iter 944/1000 - Loss: -0.277 \n",
      "Iter 945/1000 - Loss: -0.277 \n",
      "Iter 946/1000 - Loss: -0.277 \n",
      "Iter 947/1000 - Loss: -0.277 \n",
      "Iter 948/1000 - Loss: -0.277 \n",
      "Iter 949/1000 - Loss: -0.277 \n",
      "Iter 950/1000 - Loss: -0.277 \n",
      "Iter 951/1000 - Loss: -0.277 \n",
      "Iter 952/1000 - Loss: -0.277 \n",
      "Iter 953/1000 - Loss: -0.278 \n",
      "Iter 954/1000 - Loss: -0.277 \n",
      "Iter 955/1000 - Loss: -0.277 \n",
      "Iter 956/1000 - Loss: -0.277 \n",
      "Iter 957/1000 - Loss: -0.277 \n",
      "Iter 958/1000 - Loss: -0.277 \n",
      "Iter 959/1000 - Loss: -0.277 \n",
      "Iter 960/1000 - Loss: -0.277 \n",
      "Iter 961/1000 - Loss: -0.278 \n",
      "Iter 962/1000 - Loss: -0.278 \n",
      "Iter 963/1000 - Loss: -0.278 \n",
      "Iter 964/1000 - Loss: -0.278 \n",
      "Iter 965/1000 - Loss: -0.279 \n",
      "Iter 966/1000 - Loss: -0.278 \n",
      "Iter 967/1000 - Loss: -0.279 \n",
      "Iter 968/1000 - Loss: -0.279 \n",
      "Iter 969/1000 - Loss: -0.278 \n",
      "Iter 970/1000 - Loss: -0.278 \n",
      "Iter 971/1000 - Loss: -0.278 \n",
      "Iter 972/1000 - Loss: -0.278 \n",
      "Iter 973/1000 - Loss: -0.278 \n",
      "Iter 974/1000 - Loss: -0.278 \n",
      "Iter 975/1000 - Loss: -0.278 \n",
      "Iter 976/1000 - Loss: -0.279 \n",
      "Iter 977/1000 - Loss: -0.278 \n",
      "Iter 978/1000 - Loss: -0.278 \n",
      "Iter 979/1000 - Loss: -0.279 \n",
      "Iter 980/1000 - Loss: -0.279 \n",
      "Iter 981/1000 - Loss: -0.278 \n",
      "Iter 982/1000 - Loss: -0.278 \n",
      "Iter 983/1000 - Loss: -0.279 \n",
      "Iter 984/1000 - Loss: -0.278 \n",
      "Iter 985/1000 - Loss: -0.278 \n",
      "Iter 986/1000 - Loss: -0.278 \n",
      "Iter 987/1000 - Loss: -0.279 \n",
      "Iter 988/1000 - Loss: -0.278 \n",
      "Iter 989/1000 - Loss: -0.279 \n",
      "Iter 990/1000 - Loss: -0.279 \n",
      "Iter 991/1000 - Loss: -0.278 \n",
      "Iter 992/1000 - Loss: -0.278 \n",
      "Iter 993/1000 - Loss: -0.278 \n",
      "Iter 994/1000 - Loss: -0.279 \n",
      "Iter 995/1000 - Loss: -0.279 \n",
      "Iter 996/1000 - Loss: -0.278 \n",
      "Iter 997/1000 - Loss: -0.278 \n",
      "Iter 998/1000 - Loss: -0.278 \n",
      "Iter 999/1000 - Loss: -0.278 \n",
      "Iter 1000/1000 - Loss: -0.278 \n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "hogde_gp.train(model, likelihood, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.011434121988713741\n",
      "Test MSE: 0.001980280503630638\n",
      "Test R2: 0.9996825456619263\n",
      "Test MLSS: -3.012634754180908\n",
      "Test NLPD: -3.5197606086730957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultivariateNormal(loc: torch.Size([168]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hogde_gp.predict(model, likelihood, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
